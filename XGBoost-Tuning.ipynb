{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Configure the Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice I set lambda to be as small as possible and then select T by cross- validation. Performance is best when lambda is as small as possible performance with decreasing marginal utility for smaller and smaller lambda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is important to know that smaller values of shrinkage (almost) always give improved predictive performance. That is, setting shrinkage=0.001 will almost certainly result in a model with better out-of-sample predictive performance than setting shrinkage=0.01. [...] The model with shrinkage=0.001 will likely require ten times as many iterations as the model with shrinkage=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the default configuration (and presumably review learning curves?).\n",
    "2. If the system is overlearning, slow the learning down (using shrinkage?).\n",
    "3. If the system is underlearning, speed the learning up to be more aggressive (using shrinkage?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Target 500-to-1000 trees and then tune the learning rate (n estimators).\n",
    "* Set the number of samples in the leaf nodes to enough observations needed to make a good mean estimate (min child weight).\n",
    "* Configure the interaction depth to about 10 or more (max depth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of Trees (n estimators) set to a fixed value between 100 and 1000, depending on the dataset size.\n",
    "2. Learning Rate (learning rate) simplified to the ratio: [2 to 10], depending on the trees number of trees.\n",
    "3. Row Sampling (subsample) grid searched values in the range [0.5, 0.75, 1.0].\n",
    "4. Column Sampling (colsample bytree and maybe colsample bylevel) grid searched values in the range [0.4, 0.6, 0.8, 1.0].\n",
    "5. Min Leaf Weight (min child weight) simplified to the ratio 3 , where rare events rare event is the percentage of rare event observations in the dataset.\n",
    "6. Tree Size (max depth) grid searched values in the rage [4, 6, 8, 10].\n",
    "7. Min Split Gain (gamma) fixed with a value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of Trees and Learning Rate: Fix the number of trees at around 100 (rather than 1000) and then tune the learning rate.\n",
    "2. Max Tree Depth: Start with a value of 6 and presumably tune from there.\n",
    "3. Min Leaf Weight: Use a modified ratio of 1 , where rare events is the sqrt(rare events) percentage of rare event observations in the dataset.\n",
    "4. Column Sampling: Grid search values in the range of 0.3 to 0.5 (more constrained).\n",
    "5. Row Sampling: Fixed at the value 1.0.\n",
    "6. Min Split Gain: Fixed at the value 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Number and Size of Decision Trees with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Number of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XGBoost on Otto dataset, Tune n_estimators\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "n_estimators = range(50, 400, 50)\n",
    "param_grid = dict(n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(n_estimators, means, yerr=stds) \n",
    "pyplot.title(\"XGBoost n_estimators vs Log Loss\")\n",
    "pyplot.xlabel('n_estimators')\n",
    "pyplot.ylabel('Log Loss') \n",
    "pyplot.savefig('n_estimators.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Size of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In gradient boosting, we can control the size of decision trees, also called the number of layers or the depth. Shallow trees are expected to have poor performance because they capture few details of the problem and are generally referred to as weak learners. Deeper trees generally capture too many details of the problem and overfit the training dataset, limiting the ability to make good predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost on Otto dataset, Tune max_depth\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "max_depth = range(1, 11, 2)\n",
    "print(max_depth)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold,\n",
    "verbose=1)\n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(max_depth, means, yerr=stds) \n",
    "pyplot.title(\"XGBoost max_depth vs Log Loss\") \n",
    "pyplot.xlabel('max_depth')\n",
    "pyplot.ylabel('Log Loss') \n",
    "pyplot.savefig('max_depth.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune The Number and Size of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XGBoost on Otto dataset, Tune n_estimators and max_depth\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "max_depth = [2, 4, 6, 8]\n",
    "print(max_depth)\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold,\n",
    "verbose=1)\n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot results\n",
    "scores = numpy.array(means).reshape(len(max_depth), len(n_estimators))\n",
    "for i, value in enumerate(max_depth):\n",
    "  pyplot.plot(n_estimators, scores[i], label='depth: ' + str(value)) \n",
    "pyplot.legend()\n",
    "pyplot.xlabel('n_estimators')\n",
    "pyplot.ylabel('Log Loss') \n",
    "pyplot.savefig('n_estimators_vs_max_depth.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Learning Rate and Number of Trees with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost on Otto dataset, Tune learning_rate\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(learning_rate, means, yerr=stds) \n",
    "pyplot.title(\"XGBoost learning_rate vs Log Loss\") \n",
    "pyplot.xlabel('learning_rate')\n",
    "pyplot.ylabel('Log Loss') \n",
    "pyplot.savefig('learning_rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XGBoost on Otto dataset, Tune learning_rate and n_estimators\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "n_estimators = [100, 200, 300, 400, 500]\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot results\n",
    "scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators))\n",
    "for i, value in enumerate(learning_rate):\n",
    "  pyplot.plot(n_estimators, scores[i], label='learning_rate: ' + str(value)) \n",
    "pyplot.legend()\n",
    "pyplot.xlabel('n_estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Stochastic Gradient Boosting with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XGBoost on Otto dataset, tune subsample\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "param_grid = dict(subsample=subsample)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score'] \n",
    "stds = grid_result.cv_results_['std_test_score'] \n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(subsample, means, yerr=stds) \n",
    "pyplot.title(\"XGBoost subsample vs Log Loss\") \n",
    "pyplot.xlabel('subsample')\n",
    "pyplot.ylabel('Log Loss') \n",
    "pyplot.savefig('subsample.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost on Otto dataset, tune colsample_bytree\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "param_grid = dict(colsample_bytree=colsample_bytree)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(colsample_bytree, means, yerr=stds) \n",
    "pyplot.title(\"XGBoost colsample_bytree vs Log Loss\") \n",
    "pyplot.xlabel('colsample_bytree')\n",
    "pyplot.ylabel('Log Loss') \n",
    "pyplot.savefig('colsample_bytree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost on Otto dataset, tune colsample_bylevel\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "param_grid = dict(colsample_bylevel=colsample_bylevel)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(colsample_bylevel, means, yerr=stds) \n",
    "pyplot.title(\"XGBoost colsample_bylevel vs Log Loss\") \n",
    "pyplot.xlabel('colsample_bylevel')\n",
    "pyplot.ylabel('Log Loss') \n",
    "pyplot.savefig('colsample_bylevel.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
