{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Configure the Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice I set lambda to be as small as possible and then select T by cross- validation. Performance is best when lambda is as small as possible performance with decreasing marginal utility for smaller and smaller lambda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is important to know that smaller values of shrinkage (almost) always give improved predictive performance. That is, setting shrinkage=0.001 will almost certainly result in a model with better out-of-sample predictive performance than setting shrinkage=0.01. [...] The model with shrinkage=0.001 will likely require ten times as many iterations as the model with shrinkage=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the default configuration (and presumably review learning curves?).\n",
    "2. If the system is overlearning, slow the learning down (using shrinkage?).\n",
    "3. If the system is underlearning, speed the learning up to be more aggressive (using shrinkage?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Target 500-to-1000 trees and then tune the learning rate (n estimators).\n",
    "* Set the number of samples in the leaf nodes to enough observations needed to make a good mean estimate (min child weight).\n",
    "* Configure the interaction depth to about 10 or more (max depth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of Trees (n estimators) set to a fixed value between 100 and 1000, depending on the dataset size.\n",
    "2. Learning Rate (learning rate) simplified to the ratio: [2 to 10], depending on the trees number of trees.\n",
    "3. Row Sampling (subsample) grid searched values in the range [0.5, 0.75, 1.0].\n",
    "4. Column Sampling (colsample bytree and maybe colsample bylevel) grid searched values in the range [0.4, 0.6, 0.8, 1.0].\n",
    "5. Min Leaf Weight (min child weight) simplified to the ratio 3 , where rare events rare event is the percentage of rare event observations in the dataset.\n",
    "6. Tree Size (max depth) grid searched values in the rage [4, 6, 8, 10].\n",
    "7. Min Split Gain (gamma) fixed with a value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of Trees and Learning Rate: Fix the number of trees at around 100 (rather than 1000) and then tune the learning rate.\n",
    "2. Max Tree Depth: Start with a value of 6 and presumably tune from there.\n",
    "3. Min Leaf Weight: Use a modified ratio of 1 , where rare events is the sqrt(rare events) percentage of rare event observations in the dataset.\n",
    "4. Column Sampling: Grid search values in the range of 0.3 to 0.5 (more constrained).\n",
    "5. Row Sampling: Fixed at the value 1.0.\n",
    "6. Min Split Gain: Fixed at the value 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Number and Size of Decision Trees with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
